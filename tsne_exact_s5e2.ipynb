{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FuzzilyDeveloper/ML_Project/blob/master/tsne_exact_s5e2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAC4zJrIMi38"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_lnG_ki7uva"
      },
      "outputs": [],
      "source": [
        "# Install Qiskit-Aer with GPU support\n",
        "!pip install qiskit==1.2 qiskit-machine-learning qiskit-aer-gpu pandas numpy scikit-learn scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxBDTu80s3A0"
      },
      "outputs": [],
      "source": [
        "!pip install qiskit-algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5UK-XHQKySs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit.library import RealAmplitudes, ZFeatureMap\n",
        "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
        "from qiskit_aer import AerSimulator\n",
        "from qiskit.primitives import StatevectorEstimator\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4i63jdaK5mD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load data\n",
        "train_path = '/content/drive/MyDrive/playground-series-s5e2/train.csv'\n",
        "test_path = '/content/drive/MyDrive/playground-series-s5e2/test.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7iWeL1sPQuL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Downsample training data\n",
        "train_sample_size = 1500\n",
        "df_train = df_train.sample(n=train_sample_size, random_state=18)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "G615YjJMg2DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay3bJeKNPXoj"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_train_split, df_val = train_test_split(df_train, test_size=0.2, random_state=15)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_split"
      ],
      "metadata": {
        "id": "IaTEiyAqhjMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val"
      ],
      "metadata": {
        "id": "okZKN2UrhoLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0vxKQMIPqM7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Split training data\n",
        "\n",
        "# Manually select features\n",
        "categorical_cols = ['Brand', 'Material','Size', 'Laptop Compartment',\n",
        "                    'Waterproof', 'Style','Color','Compartments' ]\n",
        "numerical_cols = ['Weight Capacity (kg)']\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "        ]), categorical_cols),\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numerical_cols)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJJ9InETAe9N"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Prepare data\n",
        "X_train = df_train_split[categorical_cols + numerical_cols]\n",
        "y_train = df_train_split['Price']\n",
        "X_val = df_val[categorical_cols + numerical_cols]\n",
        "y_val = df_val['Price']\n",
        "X_test = df_test[categorical_cols + numerical_cols]\n",
        "\n",
        "# Apply preprocessing\n",
        "# X_train_processed = preprocessor.fit_transform(X_train)\n",
        "# X_val_processed = preprocessor.transform(X_val)\n",
        "# X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.fit_transform(X_val)\n",
        "X_test_processed = preprocessor.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed[0]"
      ],
      "metadata": {
        "id": "rdd2AhR-n1n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_processed[0]"
      ],
      "metadata": {
        "id": "f6FTlJehoAQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# t-SNE dimensionality reduction for training and validation\n",
        "X_train_val_processed = np.vstack([X_train_processed, X_val_processed])\n",
        "train_idx = len(X_train_processed)\n",
        "n_components = 14\n",
        "# tsne = TSNE(n_components=n_components, method='barnes_hut', random_state=42, n_iter=300)\n",
        "tsne = TSNE(n_components=n_components, method='exact', random_state=42, n_iter=300)\n",
        "X_train_val_reduced = tsne.fit_transform(X_train_val_processed)"
      ],
      "metadata": {
        "id": "6-1O8_Jl9BXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_val_processed[0]"
      ],
      "metadata": {
        "id": "2FoeoD2XjWpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_val_reduced[0]"
      ],
      "metadata": {
        "id": "e6z3pUFjkOzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split back into training and validation\n",
        "X_train_reduced = X_train_val_reduced[:train_idx]\n",
        "X_val_reduced = X_train_val_reduced[train_idx:]"
      ],
      "metadata": {
        "id": "wsdouDaE9uOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xiNUk6hGkLE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Approximate t-SNE for test set using k-NN\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "knn.fit(X_train_processed, X_train_reduced)\n",
        "X_test_reduced = knn.predict(X_test_processed)\n",
        "\n",
        "print(f\"t-SNE applied with {n_components} components (training/validation), k-NN for test set\")"
      ],
      "metadata": {
        "id": "aGcizTsr9xwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Scale target\n",
        "y_min, y_max = y_train.min(), y_train.max()"
      ],
      "metadata": {
        "id": "AsQoNHOJ92nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Quantum circuit\n",
        "num_qubits = n_components\n",
        "feature_map = ZFeatureMap(feature_dimension=num_qubits, reps=1)\n",
        "ansatz = RealAmplitudes(num_qubits=num_qubits, reps=1)\n",
        "circuit = QuantumCircuit(num_qubits)\n",
        "circuit.compose(feature_map, inplace=True)\n",
        "circuit.compose(ansatz, inplace=True)"
      ],
      "metadata": {
        "id": "SvqNiYIY-EB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simulator and QNN\n",
        "aer_simulator = AerSimulator(method='statevector', device='CPU')\n",
        "estimator = StatevectorEstimator()\n",
        "qnn = EstimatorQNN(\n",
        "    circuit=circuit,\n",
        "    estimator=estimator,\n",
        "    input_params=circuit.parameters[:num_qubits],\n",
        "    weight_params=circuit.parameters[num_qubits:]\n",
        ")"
      ],
      "metadata": {
        "id": "ZXbm2I5d-HPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training setup\n",
        "num_weights = len(qnn.weight_params)\n",
        "initial_weights = np.random.random(num_weights)\n",
        "current_weights = initial_weights.copy()\n",
        "best_weights = current_weights.copy()\n",
        "best_val_loss = float('inf')\n",
        "patience = 2\n",
        "patience_counter = 0"
      ],
      "metadata": {
        "id": "n4SrzaCg-La9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):\n",
        "    batch_num = 0\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    start_idx = 0\n",
        "    while start_idx < len(X_train_reduced):\n",
        "        end_idx = min(start_idx + 50, len(X_train_reduced))\n",
        "        X_batch = X_train_reduced[start_idx:end_idx]\n",
        "        y_batch = y_train.iloc[start_idx:end_idx] if isinstance(y_train, pd.Series) else y_train[start_idx:end_idx]\n",
        "\n",
        "        print(f\"Training on batch {batch_num + 1}\")\n",
        "        y_batch_scaled = 2 * (y_batch - y_min) / (y_max - y_min) - 1\n",
        "\n",
        "        result = minimize(\n",
        "            lambda weights: np.mean((qnn.forward(X_batch, weights).flatten() - y_batch_scaled) ** 2) + 0.005 * np.sum(weights ** 2),\n",
        "            current_weights,\n",
        "            method='COBYLA',\n",
        "            options={'maxiter': 20}\n",
        "        )\n",
        "        current_weights = result.x\n",
        "        batch_num += 1\n",
        "        start_idx += 50\n",
        "        if batch_num >= 5:\n",
        "            break\n",
        "\n",
        "    val_predictions = qnn.forward(X_val_reduced, current_weights).flatten()\n",
        "    val_scaled = 2 * (y_val - y_min) / (y_max - y_min) - 1\n",
        "    val_loss = mean_squared_error(val_scaled, val_predictions)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_weights = current_weights.copy()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "GeUPRiL6-PMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb81HZBTESSr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Training predictions\n",
        "train_predictions = []\n",
        "train_actual = []\n",
        "start_idx = 0\n",
        "while start_idx < len(X_train_reduced):\n",
        "    end_idx = min(start_idx + 50, len(X_train_reduced))\n",
        "    X_batch = X_train_reduced[start_idx:end_idx]\n",
        "    y_batch = y_train.iloc[start_idx:end_idx] if isinstance(y_train, pd.Series) else y_train[start_idx:end_idx]\n",
        "    y_pred = qnn.forward(X_batch, best_weights).flatten()\n",
        "    y_pred_scaled = y_min + (y_pred + 1) * (y_max - y_min) / 2\n",
        "    train_predictions.extend(y_pred_scaled)\n",
        "    train_actual.extend(y_batch)\n",
        "    start_idx += 50\n",
        "\n",
        "mse = mean_squared_error(train_actual, train_predictions)\n",
        "print(f\"Training MSE: {mse:.4f}\")\n",
        "\n",
        "# Test predictions in chunks\n",
        "test_predictions = []\n",
        "batch_size = 10000  # Process test set in smaller chunks\n",
        "start_idx = 0\n",
        "while start_idx < len(X_test_reduced):\n",
        "    end_idx = min(start_idx + batch_size, len(X_test_reduced))\n",
        "    X_batch = X_test_reduced[start_idx:end_idx]\n",
        "    y_pred = qnn.forward(X_batch, best_weights).flatten()\n",
        "    y_pred_scaled = y_min + (y_pred + 1) * (y_max - y_min) / 2\n",
        "    test_predictions.extend(y_pred_scaled)\n",
        "    start_idx += batch_size\n",
        "\n",
        "# Output results\n",
        "print(\"\\nSample Training Predictions (first 5):\")\n",
        "for i in range(min(5, len(train_predictions))):\n",
        "    print(f\"Actual: {train_actual[i]:.2f}, Predicted: {train_predictions[i]:.2f}\")\n",
        "\n",
        "df_test['Predicted_Price'] = test_predictions\n",
        "print(\"\\nSample Test Predictions (first 5 rows):\")\n",
        "print(df_test.head())\n",
        "\n",
        "# Ensure submission has 200,000 rows\n",
        "df_test_pred = df_test[['id', 'Predicted_Price']]\n",
        "df_test_pred.columns = ['id', 'Price']\n",
        "if len(df_test_pred) != 200000:\n",
        "    raise ValueError(f\"Submission has {len(df_test_pred)} rows, expected 200,000\")\n",
        "df_test_pred.to_csv('submission.csv', index=False)\n",
        "print(\"\\nTest predictions saved to 'submission.csv' with 200,000 rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***QSVR with quantm kernerl***"
      ],
      "metadata": {
        "id": "voj_D2frBrOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit.library import ZFeatureMap\n",
        "from qiskit.quantum_info import Statevector\n",
        "from qiskit_algorithms.utils import algorithm_globals\n",
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "import warnings\n",
        "import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Function to print timestamped messages\n",
        "def print_progress(message):\n",
        "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "algorithm_globals.random_seed = 42\n",
        "\n",
        "# Load Kaggle dataset\n",
        "print_progress(\"Loading datasets...\")\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/playground-series-s5e2/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/playground-series-s5e2/test.csv')\n",
        "\n",
        "# Remove rows with missing values from training set only\n",
        "print_progress(\"Removing missing values from training set...\")\n",
        "train_df = train_df.dropna()\n",
        "\n",
        "# Define features\n",
        "categorical_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
        "numerical_cols = ['Weight Capacity (kg)', 'Compartments']\n",
        "target = 'Price'\n",
        "\n",
        "# Impute missing values in test set\n",
        "print_progress(\"Imputing missing values in test set...\")\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "test_df[categorical_cols] = cat_imputer.fit_transform(test_df[categorical_cols])\n",
        "test_df[numerical_cols] = num_imputer.fit_transform(test_df[numerical_cols])\n",
        "\n",
        "# Preprocessing pipeline with sparse output for categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_cols),\n",
        "        ('num', StandardScaler(), numerical_cols)\n",
        "    ])\n",
        "\n",
        "# Fit and transform training data\n",
        "print_progress(\"Preprocessing training and test data...\")\n",
        "X_train = train_df[categorical_cols + numerical_cols]\n",
        "y_train = train_df[target]\n",
        "X_test = test_df[categorical_cols + numerical_cols]\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Convert sparse matrix to dense for UMAP\n",
        "X_train_preprocessed = X_train_preprocessed.toarray() if hasattr(X_train_preprocessed, 'toarray') else X_train_preprocessed\n",
        "X_test_preprocessed = X_test_preprocessed.toarray() if hasattr(X_test_preprocessed, 'toarray') else X_test_preprocessed\n",
        "\n",
        "# Subsample training data for UMAP and quantum kernel\n",
        "n_samples = 100  # Small sample for fast computation\n",
        "print_progress(f\"Subsampling {n_samples} training samples...\")\n",
        "sample_indices = np.random.choice(X_train_preprocessed.shape[0], n_samples, replace=False)\n",
        "X_train_sample = X_train_preprocessed[sample_indices]\n",
        "y_train_sample = y_train.iloc[sample_indices].values\n",
        "\n",
        "# Apply UMAP for dimensionality reduction\n",
        "n_components = 2\n",
        "print_progress(\"Applying UMAP to training sample...\")\n",
        "umap_model = umap.UMAP(n_components=n_components, random_state=42, n_jobs=1)\n",
        "X_train_transformed = umap_model.fit_transform(X_train_sample)\n",
        "print_progress(\"UMAP transformation for training sample completed.\")\n",
        "\n",
        "# Define simpler quantum feature map\n",
        "feature_map = ZFeatureMap(feature_dimension=n_components, reps=1)\n",
        "\n",
        "# Function to compute approximate quantum kernel matrix\n",
        "def quantum_kernel(X1, X2, feature_map, sample_pairs=1000):\n",
        "    kernel_matrix = np.zeros((len(X1), len(X2)))\n",
        "    # Randomly sample pairs to approximate kernel\n",
        "    n_pairs = min(sample_pairs, len(X1) * len(X2))\n",
        "    indices = np.random.choice(len(X1) * len(X2), n_pairs, replace=False)\n",
        "    for idx in indices:\n",
        "        i = idx // len(X2)\n",
        "        j = idx % len(X2)\n",
        "        qc = QuantumCircuit(feature_map.num_qubits)\n",
        "        params_i = X1[i]\n",
        "        mapped_circuit_i = feature_map.assign_parameters(params_i)\n",
        "        qc.compose(mapped_circuit_i, inplace=True)\n",
        "        params_j = X2[j]\n",
        "        mapped_circuit_j = feature_map.assign_parameters(params_j).inverse()\n",
        "        qc.compose(mapped_circuit_j, inplace=True)\n",
        "        state = Statevector.from_instruction(qc)\n",
        "        kernel_matrix[i, j] = np.abs(state.data[0])**2\n",
        "    # Fill remaining entries with average kernel value\n",
        "    if n_pairs < len(X1) * len(X2):\n",
        "        avg_kernel = np.mean(kernel_matrix[kernel_matrix != 0]) if np.any(kernel_matrix != 0) else 1.0\n",
        "        kernel_matrix[kernel_matrix == 0] = avg_kernel\n",
        "    return kernel_matrix\n",
        "\n",
        "# Compute training kernel\n",
        "print_progress(\"Computing training kernel...\")\n",
        "train_kernel = quantum_kernel(X_train_transformed, X_train_transformed, feature_map, sample_pairs=1000)\n",
        "print_progress(\"Training kernel computation completed.\")\n",
        "\n",
        "# Train QSVR model\n",
        "print_progress(\"Training QSVR model...\")\n",
        "qsvr = SVR(kernel='precomputed')\n",
        "qsvr.fit(train_kernel, y_train_sample)\n",
        "print_progress(\"QSVR model training completed.\")\n",
        "\n",
        "# Batch processing for test set\n",
        "batch_size = 500  # Small for fast processing\n",
        "y_pred_test = []\n",
        "total_batches = (X_test_preprocessed.shape[0] + batch_size - 1) // batch_size\n",
        "print_progress(f\"Starting test set prediction with {total_batches} batches...\")\n",
        "for i in range(0, X_test_preprocessed.shape[0], batch_size):\n",
        "    batch_num = i // batch_size + 1\n",
        "    print_progress(f\"Processing test batch {batch_num}/{total_batches}...\")\n",
        "    X_test_batch = X_test_preprocessed[i:i + batch_size]\n",
        "    X_test_batch_transformed = umap_model.transform(X_test_batch)\n",
        "    test_kernel_batch = quantum_kernel(X_test_batch_transformed, X_train_transformed, feature_map, sample_pairs=500)\n",
        "    y_pred_batch = qsvr.predict(test_kernel_batch)\n",
        "    y_pred_test.extend(y_pred_batch)\n",
        "print_progress(\"Test set prediction completed.\")\n",
        "\n",
        "# Convert predictions to numpy array and round to 2 decimal places\n",
        "y_pred_test = np.round(np.array(y_pred_test), 2)\n",
        "\n",
        "# Create submission file\n",
        "print_progress(\"Creating submission file...\")\n",
        "submission = pd.DataFrame({'id': test_df['id'], 'Price': y_pred_test})\n",
        "# Verify submission size\n",
        "if len(submission) != 200000:\n",
        "    print_progress(f\"Warning: Submission has {len(submission)} rows, expected 200000.\")\n",
        "else:\n",
        "    print_progress(\"Submission has correct number of rows (200000).\")\n",
        "submission.to_csv('submission_qsvr_umap_full.csv', index=False)\n",
        "print_progress(\"Submission file 'submission_qsvr_umap_full.csv' created successfully!\")\n",
        "\n",
        "# Evaluate on training sample\n",
        "print_progress(\"Evaluating training performance...\")\n",
        "y_pred_train = qsvr.predict(train_kernel)\n",
        "mse = mean_squared_error(y_train_sample, y_pred_train)\n",
        "print_progress(f\"Training Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# Plot predictions vs. true values for sampled training data\n",
        "print_progress(\"Generating training prediction plot...\")\n",
        "plt.scatter(y_train_sample, y_pred_train, color='blue', label='Predicted vs. True')\n",
        "plt.plot([y_train_sample.min(), y_train_sample.max()], [y_train_sample.min(), y_train_sample.max()], 'r--', label='Ideal')\n",
        "plt.xlabel('True Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.title('QSVR with UMAP: True vs. Predicted Prices (Training Sample)')\n",
        "plt.legend()\n",
        "plt.savefig('qsvr_backpack_umap_full_prediction.png')\n",
        "plt.close()\n",
        "print_progress(\"Plot 'qsvr_backpack_umap_full_prediction.png' saved.\")"
      ],
      "metadata": {
        "id": "m69XUzUPgZvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***quantum kernel with tsne***"
      ],
      "metadata": {
        "id": "kWVsbfJ0j1y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.manifold import TSNE\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit.library import ZFeatureMap\n",
        "from qiskit.quantum_info import Statevector\n",
        "from qiskit_algorithms.utils import algorithm_globals\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Function to print timestamped messages\n",
        "def print_progress(message):\n",
        "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "algorithm_globals.random_seed = 42\n",
        "\n",
        "# Load Kaggle dataset\n",
        "print_progress(\"Loading datasets...\")\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/playground-series-s5e2/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/playground-series-s5e2/test.csv')\n",
        "\n",
        "# Remove rows with missing values from training set only\n",
        "print_progress(\"Removing missing values from training set...\")\n",
        "train_df = train_df.dropna()\n",
        "\n",
        "# Define features\n",
        "categorical_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
        "numerical_cols = ['Weight Capacity (kg)', 'Compartments']\n",
        "target = 'Price'\n",
        "\n",
        "# Impute missing values in test set\n",
        "print_progress(\"Imputing missing values in test set...\")\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "test_df[categorical_cols] = cat_imputer.fit_transform(test_df[categorical_cols])\n",
        "test_df[numerical_cols] = num_imputer.fit_transform(test_df[numerical_cols])\n",
        "\n",
        "# Preprocessing pipeline with sparse output for categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_cols),\n",
        "        ('num', StandardScaler(), numerical_cols)\n",
        "    ])\n",
        "\n",
        "# Fit and transform training data\n",
        "print_progress(\"Preprocessing training and test data...\")\n",
        "X_train = train_df[categorical_cols + numerical_cols]\n",
        "y_train = train_df[target]\n",
        "X_test = test_df[categorical_cols + numerical_cols]\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Convert sparse matrix to dense for t-SNE\n",
        "X_train_preprocessed = X_train_preprocessed.toarray() if hasattr(X_train_preprocessed, 'toarray') else X_train_preprocessed\n",
        "X_test_preprocessed = X_test_preprocessed.toarray() if hasattr(X_test_preprocessed, 'toarray') else X_test_preprocessed\n",
        "\n",
        "# Subsample training data for t-SNE and quantum kernel\n",
        "n_samples = 100  # Small sample for fast computation\n",
        "print_progress(f\"Subsampling {n_samples} training samples...\")\n",
        "sample_indices = np.random.choice(X_train_preprocessed.shape[0], n_samples, replace=False)\n",
        "X_train_sample = X_train_preprocessed[sample_indices]\n",
        "y_train_sample = y_train.iloc[sample_indices].values\n",
        "\n",
        "# Apply t-SNE (exact method) for dimensionality reduction\n",
        "n_components = 2\n",
        "print_progress(\"Applying t-SNE (exact method) to training sample...\")\n",
        "tsne_model = TSNE(n_components=n_components, method='exact', random_state=42, n_jobs=1)\n",
        "X_train_transformed = tsne_model.fit_transform(X_train_sample)\n",
        "print_progress(\"t-SNE transformation for training sample completed.\")\n",
        "\n",
        "# Define simpler quantum feature map\n",
        "feature_map = ZFeatureMap(feature_dimension=n_components, reps=1)\n",
        "\n",
        "# Function to compute approximate quantum kernel matrix\n",
        "def quantum_kernel(X1, X2, feature_map, sample_pairs=1000):\n",
        "    kernel_matrix = np.zeros((len(X1), len(X2)))\n",
        "    # Randomly sample pairs to approximate kernel\n",
        "    n_pairs = min(sample_pairs, len(X1) * len(X2))\n",
        "    indices = np.random.choice(len(X1) * len(X2), n_pairs, replace=False)\n",
        "    for idx in indices:\n",
        "        i = idx // len(X2)\n",
        "        j = idx % len(X2)\n",
        "        qc = QuantumCircuit(feature_map.num_qubits)\n",
        "        params_i = X1[i]\n",
        "        mapped_circuit_i = feature_map.assign_parameters(params_i)\n",
        "        qc.compose(mapped_circuit_i, inplace=True)\n",
        "        params_j = X2[j]\n",
        "        mapped_circuit_j = feature_map.assign_parameters(params_j).inverse()\n",
        "        qc.compose(mapped_circuit_j, inplace=True)\n",
        "        state = Statevector.from_instruction(qc)\n",
        "        kernel_matrix[i, j] = np.abs(state.data[0])**2\n",
        "    # Fill remaining entries with average kernel value\n",
        "    if n_pairs < len(X1) * len(X2):\n",
        "        avg_kernel = np.mean(kernel_matrix[kernel_matrix != 0]) if np.any(kernel_matrix != 0) else 1.0\n",
        "        kernel_matrix[kernel_matrix == 0] = avg_kernel\n",
        "    return kernel_matrix\n",
        "\n",
        "# Compute training kernel\n",
        "print_progress(\"Computing training kernel...\")\n",
        "train_kernel = quantum_kernel(X_train_transformed, X_train_transformed, feature_map, sample_pairs=1000)\n",
        "print_progress(\"Training kernel computation completed.\")\n",
        "\n",
        "# Train QSVR model\n",
        "print_progress(\"Training QSVR model...\")\n",
        "qsvr = SVR(kernel='precomputed')\n",
        "qsvr.fit(train_kernel, y_train_sample)\n",
        "print_progress(\"QSVR model training completed.\")\n",
        "\n",
        "# Batch processing for test set\n",
        "batch_size = 500  # Small for fast processing\n",
        "y_pred_test = []\n",
        "total_batches = (X_test_preprocessed.shape[0] + batch_size - 1) // batch_size\n",
        "print_progress(f\"Starting test set prediction with {total_batches} batches...\")\n",
        "for i in range(0, X_test_preprocessed.shape[0], batch_size):\n",
        "    batch_num = i // batch_size + 1\n",
        "    print_progress(f\"Processing test batch {batch_num}/{total_batches}...\")\n",
        "    X_test_batch = X_test_preprocessed[i:i + batch_size]\n",
        "    X_test_batch_transformed = tsne_model.fit_transform(X_test_batch)\n",
        "    test_kernel_batch = quantum_kernel(X_test_batch_transformed, X_train_transformed, feature_map, sample_pairs=500)\n",
        "    y_pred_batch = qsvr.predict(test_kernel_batch)\n",
        "    y_pred_test.extend(y_pred_batch)\n",
        "print_progress(\"Test set prediction completed.\")\n",
        "\n",
        "# Convert predictions to numpy array and round to 2 decimal places\n",
        "y_pred_test = np.round(np.array(y_pred_test), 2)\n",
        "\n",
        "# Create submission file\n",
        "print_progress(\"Creating submission file...\")\n",
        "submission = pd.DataFrame({'id': test_df['id'], 'Price': y_pred_test})\n",
        "# Verify submission size\n",
        "if len(submission) != 200000:\n",
        "    print_progress(f\"Warning: Submission has {len(submission)} rows, expected 200000.\")\n",
        "else:\n",
        "    print_progress(\"Submission has correct number of rows (200000).\")\n",
        "submission.to_csv('submission_qsvr_tsne_full.csv', index=False)\n",
        "print_progress(\"Submission file 'submission_qsvr_tsne_full.csv' created successfully!\")\n",
        "\n",
        "# Evaluate on training sample\n",
        "print_progress(\"Evaluating training performance...\")\n",
        "y_pred_train = qsvr.predict(train_kernel)\n",
        "mse = mean_squared_error(y_train_sample, y_pred_train)\n",
        "print_progress(f\"Training Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# Plot predictions vs. true values for sampled training data\n",
        "print_progress(\"Generating training prediction plot...\")\n",
        "plt.scatter(y_train_sample, y_pred_train, color='blue', label='Predicted vs. True')\n",
        "plt.plot([y_train_sample.min(), y_train_sample.max()], [y_train_sample.min(), y_train_sample.max()], 'r--', label='Ideal')\n",
        "plt.xlabel('True Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.title('QSVR with t-SNE: True vs. Predicted Prices (Training Sample)')\n",
        "plt.legend()\n",
        "plt.savefig('qsvr_backpack_tsne_full_prediction.png')\n",
        "plt.close()\n",
        "print_progress(\"Plot 'qsvr_backpack_tsne_full_prediction.png' saved.\")"
      ],
      "metadata": {
        "id": "GyFdmCjLjnX4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLjAWCMzWskzaKnSm/D9ny",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}