{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FuzzilyDeveloper/ML_Project/blob/master/EDA_S5e2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6r2rMXjqTto"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EdDGD3Iq4j_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew, f_oneway\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "train_path = '/content/drive/MyDrive/playground-series-s5e2/train.csv'\n",
        "test_path = '/content/drive/MyDrive/playground-series-s5e2/test.csv'\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Define columns\n",
        "numerical_cols = ['Compartments', 'Weight Capacity (kg)']\n",
        "categorical_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j9gY-u71oY3"
      },
      "outputs": [],
      "source": [
        "# prompt: analyse the df_train and predict the price in df_test by using a gemini pretrained model\n",
        "\n",
        "# Assuming necessary libraries are already installed and imported in the preceding code.\n",
        "# If not, uncomment and run the following lines:\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the Gemini model pipeline for text classification\n",
        "# Replace 'google/flan-t5-xl' with the appropriate Gemini model identifier if available.\n",
        "# As of now, a readily accessible Gemini model via Hugging Face Transformers is not public.\n",
        "# Therefore, using a similar large language model.\n",
        "# Please check HuggingFace model hub for any Gemini models that may be available in the future.\n",
        "classifier = pipeline(\"text-classification\", model=\"google/flan-t5-xl\")\n",
        "\n",
        "# Create dummy text features from other features (replace with actual meaningful feature engineering)\n",
        "df_train['text_features'] = df_train['Brand'].astype(str) + \" \" + df_train['Material'].astype(str) + \" \" + df_train['Size'].astype(str)\n",
        "df_test['text_features'] = df_test['Brand'].astype(str) + \" \" + df_test['Material'].astype(str) + \" \" + df_test['Size'].astype(str)\n",
        "\n",
        "\n",
        "# Function to predict price based on text features\n",
        "def predict_price(text):\n",
        "  try:\n",
        "      result = classifier(text)\n",
        "      # Assuming the model output has a label and score. Adjust accordingly if needed.\n",
        "      # Here, we return the score as the price prediction. Adapt as needed for your model.\n",
        "      return result[0]['score']\n",
        "  except Exception as e:\n",
        "      print(f\"Error during prediction: {e}\")\n",
        "      return 0  # Or handle the error appropriately\n",
        "\n",
        "# Predict prices for the training set\n",
        "df_train['predicted_price'] = df_train['text_features'].apply(predict_price)\n",
        "\n",
        "# Predict prices for the test set\n",
        "df_test['predicted_price'] = df_test['text_features'].apply(predict_price)\n",
        "\n",
        "# Print the first few rows of predictions\n",
        "print(df_train[['text_features', 'predicted_price']].head())\n",
        "print(df_test[['text_features', 'predicted_price']].head())\n",
        "\n",
        "\n",
        "# Further analysis and model training (example using linear regression)\n",
        "#from sklearn.linear_model import LinearRegression\n",
        "#from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#model = LinearRegression()\n",
        "#X = df_train['predicted_price'].values.reshape(-1, 1)\n",
        "#y = df_train['Price']\n",
        "#model.fit(X, y)\n",
        "\n",
        "#df_test['Predicted_price_final'] = model.predict(df_test['predicted_price'].values.reshape(-1,1))\n",
        "\n",
        "#print(df_test['Predicted_price_final'].head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2JwQ2oE2CaC"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install pandas matplotlib seaborn scipy\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "import numpy as np\n",
        "\n",
        "# Set plot style for better visualization\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Load training data\n",
        "train_path = '/content/drive/MyDrive/playground-series-s5e2/train.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Extract Price column\n",
        "prices = df_train['Price']\n",
        "\n",
        "# Calculate skewness\n",
        "price_skewness = skew(prices)\n",
        "print(f\"Skewness of Price: {price_skewness:.3f}\")\n",
        "\n",
        "# Create figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Histogram with KDE\n",
        "sns.histplot(prices, bins=50, kde=True, color='skyblue', ax=ax1)\n",
        "ax1.set_title('Price Distribution (Histogram + KDE)')\n",
        "ax1.set_xlabel('Price')\n",
        "ax1.set_ylabel('Frequency')\n",
        "\n",
        "# Plot 2: Boxplot for spread and outliers\n",
        "sns.boxplot(x=prices, color='lightgreen', ax=ax2)\n",
        "ax2.set_title('Price Distribution (Boxplot)')\n",
        "ax2.set_xlabel('Price')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig('/content/drive/MyDrive/playground-series-s5e2/price_distribution.png')\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics for Price\n",
        "print(\"\\nPrice Summary Statistics:\")\n",
        "print(prices.describe())\n",
        "\n",
        "# Categorize prices into budget, mid-range, premium (based on quartiles)\n",
        "q25, q50, q75 = prices.quantile([0.25, 0.5, 0.75])\n",
        "print(f\"\\nPrice Ranges (based on quartiles):\")\n",
        "print(f\"Budget: <= {q25:.2f}\")\n",
        "print(f\"Mid-range: {q25:.2f} to {q75:.2f}\")\n",
        "print(f\"Premium: > {q75:.2f}\")\n",
        "\n",
        "# Count entries in each category\n",
        "budget_count = sum(prices <= q25)\n",
        "midrange_count = sum((prices > q25) & (prices <= q75))\n",
        "premium_count = sum(prices > q75)\n",
        "print(f\"\\nPrice Category Counts:\")\n",
        "print(f\"Budget: {budget_count} ({budget_count/len(prices)*100:.1f}%)\")\n",
        "print(f\"Mid-range: {midrange_count} ({midrange_count/len(prices)*100:.1f}%)\")\n",
        "print(f\"Premium: {premium_count} ({premium_count/len(prices)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rBqWRSUq_0w"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 1. Basic Data Overview\n",
        "print(\"Training Data Info:\")\n",
        "print(df_train.info())\n",
        "print(\"\\nTraining Data Head:\")\n",
        "print(df_train.head())\n",
        "print(\"\\nSummary Statistics (Numerical Columns):\")\n",
        "print(df_train[numerical_cols].describe())\n",
        "print(\"\\nValue Counts for Categorical Columns:\")\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n{col}:\\n{df_train[col].value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hlZMzL3cJ_F"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 2. Outlier Detection\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(numerical_cols + ['Price']):\n",
        "    plt.subplot(1, len(numerical_cols) + 1, i + 1)\n",
        "    sns.boxplot(y=df_train[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw3Nfu9RceNI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def detect_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
        "    return outliers\n",
        "\n",
        "for col in numerical_cols + ['Price']:\n",
        "    outliers = detect_outliers(df_train, col)\n",
        "    print(f\"\\nOutliers in {col} (count: {len(outliers)}):\\n{outliers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1dq--Mwcld0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 3. Correlation Analysis\n",
        "# Numerical correlations\n",
        "numerical_data = df_train[numerical_cols + ['Price']]\n",
        "correlation_matrix = numerical_data.corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title(\"Correlation Matrix (Numerical Features)\")\n",
        "plt.show()\n",
        "print(\"\\nCorrelations with Price (Numerical Features):\")\n",
        "print(correlation_matrix['Price'].sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA8OJrnDc9-m"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Categorical correlations using ANOVA\n",
        "print(\"\\nANOVA Test for Categorical Variables with Price:\")\n",
        "def anova_correlation(df, cat_col, target_col):\n",
        "    groups = [group[target_col].values for _, group in df.groupby(cat_col) if len(group) > 0]\n",
        "    if len(groups) > 1:\n",
        "        f_stat, p_value = f_oneway(*groups)\n",
        "        ss_between = sum(len(g) * (np.mean(g) - df[target_col].mean())**2 for g in groups)\n",
        "        ss_total = sum((df[target_col] - df[target_col].mean())**2)\n",
        "        eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
        "        return f_stat, p_value, eta_squared\n",
        "    return np.nan, np.nan, np.nan\n",
        "\n",
        "correlations = []\n",
        "for col in categorical_cols:\n",
        "    f_stat, p_value, eta_squared = anova_correlation(df_train, col, 'Price')\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
        "    print(f\"  P-value: {p_value:.4f}\")\n",
        "    print(f\"  Eta-squared: {eta_squared:.4f}\")\n",
        "    correlations.append((col, eta_squared))\n",
        "\n",
        "# Rank by eta-squared\n",
        "correlations.sort(key=lambda x: x[1] if not np.isnan(x[1]) else -1, reverse=True)\n",
        "print(\"\\nCategorical Variables Ranked by Eta-squared:\")\n",
        "for col, eta in correlations:\n",
        "    print(f\"{col}: {eta:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsJhP50wnzbd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Mean Price by categorical variable\n",
        "for col in categorical_cols:\n",
        "    grouped = df_train.groupby(col)['Price'].mean().sort_values()\n",
        "    print(f\"\\nMean Price by {col}:\\n{grouped}\")\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    grouped.plot(kind='bar')\n",
        "    plt.title(f\"Mean Price by {col}\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC8hy8wRoFz5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 4. Distribution Analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(numerical_cols + ['Price']):\n",
        "    plt.subplot(1, len(numerical_cols) + 1, i + 1)\n",
        "    sns.histplot(df_train[col], kde=True)\n",
        "    plt.title(f\"Distribution of {col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "for col in numerical_cols + ['Price']:\n",
        "    skewness = skew(df_train[col].dropna())\n",
        "    print(f\"Skewness of {col}: {skewness:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjcMUkM3qQEk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 5. Missing Values\n",
        "print(\"\\nMissing Values in Training Data:\")\n",
        "print(df_train.isnull().sum())\n",
        "print(\"\\nMissing Values in Test Data:\")\n",
        "print(df_test.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRN6MepJqnDL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 6. Feature Importance (PCA Loadings)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first', sparse_output=False))\n",
        "        ]), categorical_cols),\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numerical_cols)\n",
        "    ])\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(df_train.drop(columns=['Price']))\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "pca = PCA(n_components=12)\n",
        "X_train_reduced = pca.fit_transform(X_train_processed)\n",
        "loadings = pd.DataFrame(pca.components_.T, index=feature_names, columns=[f\"PC{i+1}\" for i in range(12)])\n",
        "print(\"\\nPCA Loadings (Top 5 features per component):\")\n",
        "for col in loadings.columns:\n",
        "    top_features = loadings[col].abs().sort_values(ascending=False).head(5)\n",
        "    print(f\"\\n{col}:\\n{top_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnvCiOHDqIrR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 7. Test Data Consistency\n",
        "for col in numerical_cols:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.kdeplot(df_train[col], label='Train')\n",
        "    sns.kdeplot(df_test[col], label='Test')\n",
        "    plt.title(f\"Distribution of {col} (Train vs Test)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nEDA Completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA_YmmKPRo-Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import spearmanr\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(8)\n",
        "\n",
        "# Load data\n",
        "train_path = '/content/drive/MyDrive/playground-series-s5e2/train.csv'\n",
        "test_path = '/content/drive/MyDrive/playground-series-s5e2/test.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(test_path)\n",
        "\n",
        "# Remove rows with missing values from df_train\n",
        "df_train = df_train.dropna()\n",
        "\n",
        "# Downsample if needed\n",
        "if len(df_train) > 5000:\n",
        "    df_train = df_train.sample(n=5000, random_state=8)\n",
        "\n",
        "# Split data\n",
        "df_train_split, df_val = train_test_split(df_train, test_size=0.2, random_state=10)\n",
        "\n",
        "# Define columns\n",
        "categorical_cols = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
        "numerical_cols = ['Compartments', 'Weight Capacity (kg)']\n",
        "target = 'Price'\n",
        "\n",
        "# Ensure target exists\n",
        "if target not in df_train.columns:\n",
        "    raise ValueError(\"Target column 'Price' not found in dataset.\")\n",
        "\n",
        "# 1. Inspect for Errors and Noise\n",
        "print(\"=== Data Summary ===\")\n",
        "print(df_train.describe(include='all'))\n",
        "print(\"\\n=== Missing Values ===\")\n",
        "print(df_train.isnull().sum())\n",
        "\n",
        "# Outliers: Boxplot for numerical columns and target\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(numerical_cols + [target], 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.boxplot(y=df_train[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Analyze Feature Relevance\n",
        "# Encode categorical variables for correlation/feature importance\n",
        "df_encoded = pd.get_dummies(df_train[categorical_cols], drop_first=True)\n",
        "df_encoded = pd.concat([df_train[numerical_cols + [target]], df_encoded], axis=1)\n",
        "\n",
        "# Spearman correlation (suitable for non-linear relationships)\n",
        "correlations = {}\n",
        "for col in df_encoded.columns:\n",
        "    if col != target:\n",
        "        corr, _ = spearmanr(df_encoded[col], df_encoded[target])\n",
        "        correlations[col] = corr\n",
        "print(\"\\n=== Spearman Correlations with Price ===\")\n",
        "for col, corr in sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True):\n",
        "    print(f\"{col}: {corr:.3f}\")\n",
        "\n",
        "# Feature Importance via Random Forest\n",
        "X = df_encoded.drop(columns=[target])\n",
        "y = df_encoded[target]\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=8)\n",
        "rf.fit(X, y)\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(\"\\n=== Feature Importance (Random Forest) ===\")\n",
        "print(importances)\n",
        "\n",
        "# 3. Examine Data Distribution\n",
        "# Histograms for numerical columns and target\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(numerical_cols + [target], 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.histplot(df_train[col], kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check skewness\n",
        "print(\"\\n=== Skewness ===\")\n",
        "for col in numerical_cols + [target]:\n",
        "    print(f\"{col}: {df_train[col].skew():.3f}\")\n",
        "\n",
        "# 4. Assess Dataset Size and Diversity\n",
        "print(f\"\\n=== Dataset Size ===\")\n",
        "print(f\"Training set: {len(df_train)} rows\")\n",
        "print(f\"Unique values in categorical columns:\")\n",
        "for col in categorical_cols:\n",
        "    print(f\"{col}: {df_train[col].nunique()} unique values\")\n",
        "\n",
        "# 5. Check for Multicollinearity (numerical features)\n",
        "X_num = df_train[numerical_cols]\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = numerical_cols\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_num.values, i) for i in range(X_num.shape[1])]\n",
        "print(\"\\n=== Variance Inflation Factor (VIF) ===\")\n",
        "print(vif_data)\n",
        "\n",
        "# 6. Cross-Validation for Consistency\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=8)\n",
        "mse_scores = []\n",
        "for train_idx, val_idx in kf.split(df_train):\n",
        "    train_fold = df_train.iloc[train_idx]\n",
        "    val_fold = df_train.iloc[val_idx]\n",
        "\n",
        "    # Simple preprocessing for CV\n",
        "    X_train = pd.get_dummies(train_fold[categorical_cols], drop_first=True)\n",
        "    X_train = pd.concat([train_fold[numerical_cols], X_train], axis=1)\n",
        "    y_train = train_fold[target]\n",
        "    X_val = pd.get_dummies(val_fold[categorical_cols], drop_first=True)\n",
        "    X_val = pd.concat([val_fold[numerical_cols], X_val], axis=1)\n",
        "    y_val = val_fold[target]\n",
        "\n",
        "    # Align columns\n",
        "    X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "    # Train model\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=8)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    mse = np.mean((y_val - y_pred) ** 2)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "print(\"\\n=== Cross-Validation MSE ===\")\n",
        "print(f\"MSE per fold: {mse_scores}\")\n",
        "print(f\"Mean MSE: {np.mean(mse_scores):.3f}, Std: {np.std(mse_scores):.3f}\")\n",
        "\n",
        "# 7. Baseline Comparison\n",
        "baseline_pred = np.mean(df_train[target])\n",
        "baseline_mse = np.mean((df_train[target] - baseline_pred) ** 2)\n",
        "print(\"\\n=== Baseline MSE (Mean Predictor) ===\")\n",
        "print(f\"Baseline MSE: {baseline_mse:.3f}\")\n",
        "\n",
        "# 8. Visualize Feature-Target Relationships\n",
        "# Scatter plots for numerical features vs. Price\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, col in enumerate(numerical_cols, 1):\n",
        "    plt.subplot(1, 2, i)\n",
        "    sns.scatterplot(x=df_train[col], y=df_train[target])\n",
        "    plt.title(f\"{col} vs. Price\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Boxplots for categorical features vs. Price\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(categorical_cols, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.boxplot(x=df_train[col], y=df_train[target])\n",
        "    plt.title(f\"{col} vs. Price\")\n",
        "    plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMzm/IqIku+4dwb/C/Jl44",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}